+++
draft = false
+++

# Publications

## 2024

<sub><div style="line-height: 1.2;">
[28] [IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models](https://arxiv.org/abs/2403.15952)  
    Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, _Yue Dong_, Rifat Shahriyar  
    <i>Conference on Language Modeling (COLM) 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[27] [Cross-task defense: Instruction-tuning LLMs for content safety](https://arxiv.org/abs/2312.06924)  
    Yu Fu, Wen Xiao, Jia Chen, Jiachen Li, Evangelos Papalexakis, Aichi Chien, _Yue Dong_  
    <i>TrustNLP Workshop @ NAACL 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[26] [Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack](https://arxiv.org/abs/2312.06924)  
    Yu Fu, Yufei Li, Wen Xiao, Cong Liu, _Yue Dong_  
    <i>ACL 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[25] [Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks](https://arxiv.org/abs/2312.14440)  
    Haz Sameen Shahgir, Xianghao Kong, Greg Ver Steeg, _Yue Dong_  
    <i>ACL Findings 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[24] [Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset](https://arxiv.org/abs/2311.09443)  
    Brooklyn Sheppard, Anna Richter, Allison Cohen, Elizabeth Allyn Smith, Tamara Kneese, Carolyne Pelletier, Ioana Baldini, _Yue Dong_  
    <i>ACL Findings 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[23] [Source-Free Domain Adaptation for Question Answering with Masked Self-training](https://arxiv.org/abs/2212.09563)  
    Maxwell Yin, Boyu Wang, _Yue Dong_, Charles Ling  
    <i>TACL 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[22] [PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering](https://arxiv.org/abs/2402.11034)  
    Jannat Ara Meem, Muhammad Shihab Rashid, _Yue Dong_, Vagelis Hristidis  
    <i>ACL Findings 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[21] [EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models](https://arxiv.org/abs/2402.10866)  
    Muhammad Shihab Rashid, Jannat Ara Meem, _Yue Dong_, Vagelis Hristidis  
    <i>ACL Findings 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[20] [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://arxiv.org/abs/2401.12345)  
    Erfan Shayegani, _Yue Dong_, Nael Abu-Ghazaleh  
    <i>ICLR 2024</i>
</div></sub>

<sub><div style="line-height: 1.2;">
[19] [Watermarking conditional text generation for AI detection: Unveiling challenges and a semantic-aware watermark remedy](https://arxiv.org/abs/2401.67890)  
    Yu Fu, Deyi Xiong, _Yue Dong_  
    <i>AAAI 2024</i>
</div></sub>

### Pre-prints

<sub><div style="line-height: 1.2;">
- [TRAWL: Tensor Reduced and Approximated Weights for Large Language Models](https://arxiv.org/abs/2406.17261)  
    Y Luo, H Patel, Y Fu, D Ahn, J Chen, _Yue Dong_, EE Papalexakis  
    <i>arXiv preprint arXiv:2406.17261</i>
</div></sub>

<sub><div style="line-height: 1.2;">
- [Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources](https://arxiv.org/abs/2406.07136)  
    MS Rashid, JA Meem, _Yue Dong_, V Hristidis  
    <i>arXiv preprint arXiv:2406.07136</i>
</div></sub>

<sub><div style="line-height: 1.2;">
- [PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/abs/2406.02069)  
    Y Zhang, B Gao, T Liu, K Lu, W Xiong, _Yue Dong_, B Chang, J Hu, W Xiao  
    <i>arXiv preprint arXiv:2406.02069</i>
</div></sub>

<sub><div style="line-height: 1.2;">
- [Cross-Modal Safety Alignment: Is textual unlearning all you need?](https://arxiv.org/abs/2406.02575)  
    T Chakraborty, E Shayegani, Z Cai, N Abu-Ghazaleh, MS Asif, _Yue Dong_  
    <i>arXiv preprint arXiv:2406.02575</i>
</div></sub>

<sub><div style="line-height: 1.2;">
- [Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study](https://arxiv.org/abs/2402.05939)  
    Y Li, S Chen, Y Guo, W Yang, _Yue Dong_, C Liu  
    <i>arXiv preprint arXiv:2402.05939</i>
</div></sub>

<sub><div style="line-height: 1.2;">
- [BiRNA-BERT Allows Efficient RNA Language Modeling with Adaptive Tokenization](https://biorxiv.org/content/early/2024/07/02/601703)  
    MT Tahmid, HS Shahgir, S Mahbub, _Yue Dong_, MS Bayzid  
    <i>bioRxiv</i>
</div></sub>

<sub><div style="line-height: 1.2;">
- [Mechanisms of non-factual hallucinations in language models](https://arxiv.org/abs/2403.18167)  
    L Yu, M Cao, JCK Cheung, _Yue Dong_  
    <i>arXiv preprint arXiv:2403.18167</i>
</div></sub>

### 2023
[18] Survey of vulnerabilities in large language models revealed by adversarial attacks
[17] Inverse Reinforcement Learning for Text Summarization


### 2022
[16] Learning with Rejection for Abstractive Text Summarization
[15] Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization
[14] Faithful to the document or to the world? mitigating hallucinations via entity-linked knowledge in abstractive summarization

### 2021
[13] On-the-fly attention modulation for neural generation
[12] Bringing structure into summaries: a faceted summarization dataset for long scientific documents
[11] Discourse-Aware Unsupervised Summarization of Long Scientific Documents


### 2020

[10] Factual error correction for abstractive summarization models
[9]  Multi-XScience: A large-scale dataset for extreme multi-document summarization of scientific articles
[8] Multi-fact correction in abstractive text summarization

### 2019 
[7] Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses
[6] Learning multi-task communication with message passing for sequence learning
[5] EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing

### Before 2018 
[4]  Banditsum: Extractive summarization as a contextual bandit
[3] Threaded ensembles of autoencoders for stream learning
[2] A hierarchical neural attention-based text classifier
[1] Threaded ensembles of supervised and unsupervised neural networks for stream learning
 
